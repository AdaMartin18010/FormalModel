# M3DTæ¡†æ¶å®ç°ç¤ºä¾‹ / M3DT Framework Implementation Example

## æ¦‚è¿° / Overview

æœ¬æ–‡æ¡£å±•ç¤ºäº†2025å¹´æœ€æ–°M3DTï¼ˆMixed Expert Decision Transformerï¼‰æ¡†æ¶çš„å®Œæ•´å®ç°ç¤ºä¾‹ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ··åˆä¸“å®¶(MoE)ç»“æ„å¢å¼ºå†³ç­–å˜æ¢å™¨ï¼ŒæˆåŠŸæ‰©å±•è‡³160ä¸ªä»»åŠ¡ï¼Œå±•ç¤ºäº†å“è¶Šçš„ä»»åŠ¡æ‰©å±•æ€§å’Œæ€§èƒ½ã€‚

## ğŸ¯ M3DTæ¡†æ¶åŸç† / M3DT Framework Principles

### æ ¸å¿ƒæ€æƒ³ / Core Concept

M3DTæ¡†æ¶ç»“åˆäº†æ··åˆä¸“å®¶æ¶æ„å’Œå†³ç­–å˜æ¢å™¨çš„ä¼˜åŠ¿ï¼Œé€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒæœºåˆ¶å®ç°å¤§è§„æ¨¡å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼Œèƒ½å¤Ÿå¤„ç†160ä¸ªä¸åŒçš„ä»»åŠ¡ã€‚

### æ•°å­¦å½¢å¼åŒ– / Mathematical Formulation

#### 1. å†³ç­–å˜æ¢å™¨åŸºç¡€ / Decision Transformer Foundation

**çŠ¶æ€åºåˆ—**:
$$S = (s_1, s_2, \ldots, s_T)$$

**åŠ¨ä½œåºåˆ—**:
$$A = (a_1, a_2, \ldots, a_T)$$

**å¥–åŠ±åºåˆ—**:
$$R = (r_1, r_2, \ldots, r_T)$$

**è½¨è¿¹è¡¨ç¤º**:
$$\tau = (s_1, a_1, r_1, s_2, a_2, r_2, \ldots, s_T, a_T, r_T)$$

#### 2. æ··åˆä¸“å®¶æ¶æ„ / Mixture of Experts Architecture

**ä¸“å®¶ç½‘ç»œ**:
$$E = \{E_1, E_2, \ldots, E_K\}$$

**é—¨æ§ç½‘ç»œ**:
$$G: \mathbb{R}^d \rightarrow \mathbb{R}^K$$

**é—¨æ§æƒé‡**:
$$g(x) = \text{softmax}(G(x))$$

**ä¸“å®¶è¾“å‡º**:
$$y = \sum_{i=1}^K g_i(x) \cdot E_i(x)$$

#### 3. ä¸‰é˜¶æ®µè®­ç»ƒæœºåˆ¶ / Three-Stage Training Mechanism

**é˜¶æ®µ1: é¢„è®­ç»ƒé˜¶æ®µ**
$$\mathcal{L}_{pretrain} = \mathbb{E}_{\tau}[\log p(a_t | s_t, a_{<t}, r_{<t})]$$

**é˜¶æ®µ2: ä¸“å®¶è®­ç»ƒé˜¶æ®µ**
$$\mathcal{L}_{expert} = \mathbb{E}_{(s,a,r)}[\|E_i(s) - a\|_2^2]$$

**é˜¶æ®µ3: è”åˆä¼˜åŒ–é˜¶æ®µ**
$$\mathcal{L}_{joint} = \mathcal{L}_{pretrain} + \lambda \mathcal{L}_{expert}$$

## ğŸ”§ ç®—æ³•å®ç° / Algorithm Implementation

### Pythonå®ç° / Python Implementation

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import List, Dict, Tuple, Optional
import math
from dataclasses import dataclass

@dataclass
class M3DTConfig:
    """M3DTé…ç½®ç±»"""
    state_dim: int = 64
    action_dim: int = 32
    reward_dim: int = 1
    hidden_dim: int = 512
    num_layers: int = 6
    num_heads: int = 8
    num_experts: int = 8
    expert_capacity: int = 4
    dropout: float = 0.1
    max_seq_len: int = 1000
    learning_rate: float = 1e-4
    weight_decay: float = 1e-5
    warmup_steps: int = 1000

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    def __init__(self, d_model: int, max_len: int = 1000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.pe[:x.size(0), :]

class ExpertNetwork(nn.Module):
    """ä¸“å®¶ç½‘ç»œ"""
    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, output_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)

class GatingNetwork(nn.Module):
    """é—¨æ§ç½‘ç»œ"""
    def __init__(self, input_dim: int, num_experts: int):
        super().__init__()
        self.gate = nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.ReLU(),
            nn.Linear(input_dim // 2, num_experts),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.gate(x)

class MixtureOfExperts(nn.Module):
    """æ··åˆä¸“å®¶æ¨¡å—"""
    def __init__(self, input_dim: int, output_dim: int, 
                 num_experts: int, expert_capacity: int):
        super().__init__()
        self.num_experts = num_experts
        self.expert_capacity = expert_capacity
        
        # ä¸“å®¶ç½‘ç»œ
        self.experts = nn.ModuleList([
            ExpertNetwork(input_dim, output_dim, input_dim * 2)
            for _ in range(num_experts)
        ])
        
        # é—¨æ§ç½‘ç»œ
        self.gate = GatingNetwork(input_dim, num_experts)
        
        # å™ªå£°å±‚ï¼ˆç”¨äºè®­ç»ƒæ—¶çš„éšæœºæ€§ï¼‰
        self.noise = nn.Linear(input_dim, num_experts)
    
    def forward(self, x: torch.Tensor, training: bool = True) -> torch.Tensor:
        batch_size, seq_len, _ = x.shape
        
        # è®¡ç®—é—¨æ§æƒé‡
        gate_logits = self.gate(x)
        
        if training:
            # æ·»åŠ å™ªå£°è¿›è¡Œè®­ç»ƒ
            noise = torch.randn_like(gate_logits) * 0.1
            gate_logits = gate_logits + noise
        
        gate_weights = F.softmax(gate_logits, dim=-1)
        
        # é€‰æ‹©top-kä¸“å®¶
        top_k = min(self.expert_capacity, self.num_experts)
        top_k_weights, top_k_indices = torch.topk(gate_weights, top_k, dim=-1)
        
        # é‡æ–°å½’ä¸€åŒ–æƒé‡
        top_k_weights = top_k_weights / top_k_weights.sum(dim=-1, keepdim=True)
        
        # è®¡ç®—ä¸“å®¶è¾“å‡º
        expert_outputs = []
        for i in range(self.num_experts):
            expert_output = self.experts[i](x)
            expert_outputs.append(expert_output)
        
        expert_outputs = torch.stack(expert_outputs, dim=-2)  # [batch, seq, num_experts, output_dim]
        
        # åŠ æƒç»„åˆä¸“å®¶è¾“å‡º
        output = torch.zeros_like(expert_outputs[:, :, 0, :])
        for k in range(top_k):
            expert_idx = top_k_indices[:, :, k]
            expert_weight = top_k_weights[:, :, k:k+1]
            
            # ä½¿ç”¨gatheré€‰æ‹©å¯¹åº”ä¸“å®¶çš„è¾“å‡º
            selected_output = torch.gather(
                expert_outputs, dim=-2, 
                index=expert_idx.unsqueeze(-1).expand(-1, -1, -1, expert_outputs.size(-1))
            ).squeeze(-2)
            
            output += expert_weight * selected_output
        
        return output

class M3DTBlock(nn.Module):
    """M3DT Transformerå—"""
    def __init__(self, config: M3DTConfig):
        super().__init__()
        self.config = config
        
        # å¤šå¤´è‡ªæ³¨æ„åŠ›
        self.self_attn = nn.MultiheadAttention(
            config.hidden_dim, config.num_heads, 
            dropout=config.dropout, batch_first=True
        )
        
        # æ··åˆä¸“å®¶å‰é¦ˆç½‘ç»œ
        self.moe_ffn = MixtureOfExperts(
            config.hidden_dim, config.hidden_dim,
            config.num_experts, config.expert_capacity
        )
        
        # å±‚å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(config.hidden_dim)
        self.norm2 = nn.LayerNorm(config.hidden_dim)
        
        # Dropout
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # è‡ªæ³¨æ„åŠ›
        attn_output, _ = self.self_attn(x, x, x, attn_mask=mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # æ··åˆä¸“å®¶å‰é¦ˆ
        moe_output = self.moe_ffn(x)
        x = self.norm2(x + self.dropout(moe_output))
        
        return x

class M3DTModel(nn.Module):
    """M3DTä¸»æ¨¡å‹"""
    def __init__(self, config: M3DTConfig):
        super().__init__()
        self.config = config
        
        # è¾“å…¥åµŒå…¥
        self.state_embedding = nn.Linear(config.state_dim, config.hidden_dim)
        self.action_embedding = nn.Linear(config.action_dim, config.hidden_dim)
        self.reward_embedding = nn.Linear(config.reward_dim, config.hidden_dim)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(config.hidden_dim, config.max_seq_len)
        
        # Transformerå±‚
        self.transformer_layers = nn.ModuleList([
            M3DTBlock(config) for _ in range(config.num_layers)
        ])
        
        # è¾“å‡ºå±‚
        self.action_head = nn.Linear(config.hidden_dim, config.action_dim)
        self.value_head = nn.Linear(config.hidden_dim, 1)
        
        # ä»»åŠ¡åµŒå…¥ï¼ˆç”¨äºå¤šä»»åŠ¡å­¦ä¹ ï¼‰
        self.task_embedding = nn.Embedding(160, config.hidden_dim)  # æ”¯æŒ160ä¸ªä»»åŠ¡
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(self, states: torch.Tensor, actions: torch.Tensor, 
                rewards: torch.Tensor, task_ids: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        Args:
            states: [batch_size, seq_len, state_dim]
            actions: [batch_size, seq_len, action_dim]
            rewards: [batch_size, seq_len, reward_dim]
            task_ids: [batch_size]
            mask: [batch_size, seq_len, seq_len]
        """
        batch_size, seq_len = states.shape[:2]
        
        # åµŒå…¥
        state_emb = self.state_embedding(states)
        action_emb = self.action_embedding(actions)
        reward_emb = self.reward_embedding(rewards)
        
        # ä»»åŠ¡åµŒå…¥
        task_emb = self.task_embedding(task_ids).unsqueeze(1)  # [batch_size, 1, hidden_dim]
        
        # ç»„åˆåµŒå…¥ï¼ˆçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±äº¤æ›¿ï¼‰
        embeddings = []
        for t in range(seq_len):
            embeddings.append(state_emb[:, t])
            embeddings.append(action_emb[:, t])
            embeddings.append(reward_emb[:, t])
        
        x = torch.stack(embeddings, dim=1)  # [batch_size, 3*seq_len, hidden_dim]
        
        # æ·»åŠ ä»»åŠ¡åµŒå…¥
        x = x + task_emb
        
        # ä½ç½®ç¼–ç 
        x = self.pos_encoding(x.transpose(0, 1)).transpose(0, 1)
        
        # Transformerå±‚
        for layer in self.transformer_layers:
            x = layer(x, mask)
        
        # è¾“å‡ºé¢„æµ‹
        actions_pred = self.action_head(x)
        values = self.value_head(x)
        
        return actions_pred, values

class M3DTTrainer:
    """M3DTè®­ç»ƒå™¨"""
    def __init__(self, model: M3DTModel, config: M3DTConfig):
        self.model = model
        self.config = config
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.LinearLR(
            self.optimizer, 
            start_factor=0.1,
            total_iters=config.warmup_steps
        )
        
        # æŸå¤±å‡½æ•°
        self.action_loss_fn = nn.MSELoss()
        self.value_loss_fn = nn.MSELoss()
    
    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """å•æ­¥è®­ç»ƒ"""
        self.model.train()
        self.optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        actions_pred, values_pred = self.model(
            batch['states'],
            batch['actions'],
            batch['rewards'],
            batch['task_ids'],
            batch.get('mask', None)
        )
        
        # è®¡ç®—æŸå¤±
        action_loss = self.action_loss_fn(actions_pred, batch['target_actions'])
        value_loss = self.value_loss_fn(values_pred, batch['target_values'])
        
        total_loss = action_loss + 0.5 * value_loss
        
        # åå‘ä¼ æ’­
        total_loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        self.scheduler.step()
        
        return {
            'total_loss': total_loss.item(),
            'action_loss': action_loss.item(),
            'value_loss': value_loss.item(),
            'learning_rate': self.scheduler.get_last_lr()[0]
        }
    
    def evaluate(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """è¯„ä¼°"""
        self.model.eval()
        
        with torch.no_grad():
            actions_pred, values_pred = self.model(
                batch['states'],
                batch['actions'],
                batch['rewards'],
                batch['task_ids'],
                batch.get('mask', None)
            )
            
            action_loss = self.action_loss_fn(actions_pred, batch['target_actions'])
            value_loss = self.value_loss_fn(values_pred, batch['target_values'])
            
            # è®¡ç®—å‡†ç¡®ç‡ï¼ˆåŠ¨ä½œé¢„æµ‹ï¼‰
            action_accuracy = self._compute_accuracy(actions_pred, batch['target_actions'])
            
            return {
                'action_loss': action_loss.item(),
                'value_loss': value_loss.item(),
                'action_accuracy': action_accuracy
            }
    
    def _compute_accuracy(self, pred: torch.Tensor, target: torch.Tensor) -> float:
        """è®¡ç®—åŠ¨ä½œé¢„æµ‹å‡†ç¡®ç‡"""
        pred_actions = torch.argmax(pred, dim=-1)
        target_actions = torch.argmax(target, dim=-1)
        accuracy = (pred_actions == target_actions).float().mean().item()
        return accuracy

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # é…ç½®
    config = M3DTConfig(
        state_dim=64,
        action_dim=32,
        reward_dim=1,
        hidden_dim=512,
        num_layers=6,
        num_heads=8,
        num_experts=8,
        expert_capacity=4
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = M3DTModel(config)
    trainer = M3DTTrainer(model, config)
    
    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 16
    seq_len = 100
    
    batch = {
        'states': torch.randn(batch_size, seq_len, config.state_dim),
        'actions': torch.randn(batch_size, seq_len, config.action_dim),
        'rewards': torch.randn(batch_size, seq_len, config.reward_dim),
        'task_ids': torch.randint(0, 160, (batch_size,)),
        'target_actions': torch.randn(batch_size, 3*seq_len, config.action_dim),
        'target_values': torch.randn(batch_size, 3*seq_len, 1)
    }
    
    # è®­ç»ƒ
    for epoch in range(10):
        loss_info = trainer.train_step(batch)
        print(f"Epoch {epoch}: {loss_info}")
    
    # è¯„ä¼°
    eval_info = trainer.evaluate(batch)
    print(f"Evaluation: {eval_info}")

if __name__ == "__main__":
    main()
```

### Rustå®ç° / Rust Implementation

```rust
use ndarray::{Array2, Array3, Array4, Axis};
use ndarray_rand::RandomExt;
use rand::distributions::Uniform;
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct M3DTConfig {
    pub state_dim: usize,
    pub action_dim: usize,
    pub reward_dim: usize,
    pub hidden_dim: usize,
    pub num_layers: usize,
    pub num_heads: usize,
    pub num_experts: usize,
    pub expert_capacity: usize,
    pub dropout: f64,
    pub max_seq_len: usize,
    pub learning_rate: f64,
    pub weight_decay: f64,
    pub warmup_steps: usize,
}

impl Default for M3DTConfig {
    fn default() -> Self {
        Self {
            state_dim: 64,
            action_dim: 32,
            reward_dim: 1,
            hidden_dim: 512,
            num_layers: 6,
            num_heads: 8,
            num_experts: 8,
            expert_capacity: 4,
            dropout: 0.1,
            max_seq_len: 1000,
            learning_rate: 1e-4,
            weight_decay: 1e-5,
            warmup_steps: 1000,
        }
    }
}

#[derive(Debug, Clone)]
pub struct PositionalEncoding {
    pub pe: Array2<f64>,
    pub max_len: usize,
    pub d_model: usize,
}

impl PositionalEncoding {
    pub fn new(d_model: usize, max_len: usize) -> Self {
        let mut pe = Array2::zeros((max_len, d_model));
        
        for pos in 0..max_len {
            for i in (0..d_model).step_by(2) {
                let div_term = (-2.0 * (i as f64) * std::f64::consts::LN_2 / d_model as f64).exp();
                pe[[pos, i]] = (pos as f64 * div_term).sin();
                if i + 1 < d_model {
                    pe[[pos, i + 1]] = (pos as f64 * div_term).cos();
                }
            }
        }
        
        Self { pe, max_len, d_model }
    }
    
    pub fn forward(&self, x: &Array3<f64>) -> Array3<f64> {
        let (seq_len, batch_size, d_model) = x.dim();
        assert_eq!(d_model, self.d_model);
        
        let mut output = x.clone();
        for t in 0..seq_len {
            for b in 0..batch_size {
                output.slice_mut(s![t, b, ..]).assign(&x.slice(s![t, b, ..]) + &self.pe.slice(s![t, ..]));
            }
        }
        
        output
    }
}

#[derive(Debug, Clone)]
pub struct ExpertNetwork {
    pub weights: Vec<Array2<f64>>,
    pub biases: Vec<Array1<f64>>,
    pub input_dim: usize,
    pub output_dim: usize,
}

impl ExpertNetwork {
    pub fn new(input_dim: usize, output_dim: usize, hidden_dim: usize) -> Self {
        let mut weights = Vec::new();
        let mut biases = Vec::new();
        
        // ç¬¬ä¸€å±‚: input_dim -> hidden_dim
        weights.push(Array2::random((input_dim, hidden_dim), Uniform::new(-0.1, 0.1)));
        biases.push(Array1::zeros(hidden_dim));
        
        // ç¬¬äºŒå±‚: hidden_dim -> hidden_dim
        weights.push(Array2::random((hidden_dim, hidden_dim), Uniform::new(-0.1, 0.1)));
        biases.push(Array1::zeros(hidden_dim));
        
        // ç¬¬ä¸‰å±‚: hidden_dim -> output_dim
        weights.push(Array2::random((hidden_dim, output_dim), Uniform::new(-0.1, 0.1)));
        biases.push(Array1::zeros(output_dim));
        
        Self {
            weights,
            biases,
            input_dim,
            output_dim,
        }
    }
    
    pub fn forward(&self, x: &Array3<f64>) -> Array3<f64> {
        let mut output = x.clone();
        
        for (weight, bias) in self.weights.iter().zip(self.biases.iter()) {
            // é‡å¡‘ä¸º2Dè¿›è¡ŒçŸ©é˜µä¹˜æ³•
            let (seq_len, batch_size, input_dim) = output.dim();
            let reshaped = output.into_shape((seq_len * batch_size, input_dim)).unwrap();
            let result = reshaped.dot(weight) + bias;
            output = result.into_shape((seq_len, batch_size, weight.ncols())).unwrap();
            
            // ReLUæ¿€æ´»
            output.mapv_inplace(|v| if v > 0.0 { v } else { 0.0 });
        }
        
        output
    }
}

#[derive(Debug, Clone)]
pub struct GatingNetwork {
    pub weights: Vec<Array2<f64>>,
    pub biases: Vec<Array1<f64>>,
    pub input_dim: usize,
    pub num_experts: usize,
}

impl GatingNetwork {
    pub fn new(input_dim: usize, num_experts: usize) -> Self {
        let mut weights = Vec::new();
        let mut biases = Vec::new();
        
        // ç¬¬ä¸€å±‚: input_dim -> input_dim/2
        weights.push(Array2::random((input_dim, input_dim / 2), Uniform::new(-0.1, 0.1)));
        biases.push(Array1::zeros(input_dim / 2));
        
        // ç¬¬äºŒå±‚: input_dim/2 -> num_experts
        weights.push(Array2::random((input_dim / 2, num_experts), Uniform::new(-0.1, 0.1)));
        biases.push(Array1::zeros(num_experts));
        
        Self {
            weights,
            biases,
            input_dim,
            num_experts,
        }
    }
    
    pub fn forward(&self, x: &Array3<f64>) -> Array3<f64> {
        let mut output = x.clone();
        
        for (weight, bias) in self.weights.iter().zip(self.biases.iter()) {
            let (seq_len, batch_size, input_dim) = output.dim();
            let reshaped = output.into_shape((seq_len * batch_size, input_dim)).unwrap();
            let result = reshaped.dot(weight) + bias;
            output = result.into_shape((seq_len, batch_size, weight.ncols())).unwrap();
            
            // ReLUæ¿€æ´»ï¼ˆé™¤äº†æœ€åä¸€å±‚ï¼‰
            if weight.ncols() != self.num_experts {
                output.mapv_inplace(|v| if v > 0.0 { v } else { 0.0 });
            }
        }
        
        // æœ€åä¸€å±‚ä½¿ç”¨softmax
        self.softmax(&mut output);
        output
    }
    
    fn softmax(&self, x: &mut Array3<f64>) {
        for mut row in x.axis_iter_mut(Axis(0)) {
            for mut batch in row.axis_iter_mut(Axis(0)) {
                let max_val = batch.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
                batch.mapv_inplace(|v| (v - max_val).exp());
                let sum: f64 = batch.sum();
                batch.mapv_inplace(|v| v / sum);
            }
        }
    }
}

#[derive(Debug, Clone)]
pub struct MixtureOfExperts {
    pub experts: Vec<ExpertNetwork>,
    pub gate: GatingNetwork,
    pub num_experts: usize,
    pub expert_capacity: usize,
}

impl MixtureOfExperts {
    pub fn new(input_dim: usize, output_dim: usize, 
               num_experts: usize, expert_capacity: usize) -> Self {
        let experts = (0..num_experts)
            .map(|_| ExpertNetwork::new(input_dim, output_dim, input_dim * 2))
            .collect();
        
        let gate = GatingNetwork::new(input_dim, num_experts);
        
        Self {
            experts,
            gate,
            num_experts,
            expert_capacity,
        }
    }
    
    pub fn forward(&self, x: &Array3<f64>) -> Array3<f64> {
        let (seq_len, batch_size, input_dim) = x.dim();
        
        // è®¡ç®—é—¨æ§æƒé‡
        let gate_weights = self.gate.forward(x);
        
        // é€‰æ‹©top-kä¸“å®¶
        let top_k = self.expert_capacity.min(self.num_experts);
        
        // è®¡ç®—ä¸“å®¶è¾“å‡º
        let mut expert_outputs = Vec::new();
        for expert in &self.experts {
            let output = expert.forward(x);
            expert_outputs.push(output);
        }
        
        // åŠ æƒç»„åˆä¸“å®¶è¾“å‡º
        let mut final_output = Array3::zeros((seq_len, batch_size, self.experts[0].output_dim));
        
        for t in 0..seq_len {
            for b in 0..batch_size {
                let mut weighted_sum = Array1::zeros(self.experts[0].output_dim);
                let mut total_weight = 0.0;
                
                // é€‰æ‹©top-kä¸“å®¶
                let mut expert_weights: Vec<(usize, f64)> = (0..self.num_experts)
                    .map(|i| (i, gate_weights[[t, b, i]]))
                    .collect();
                expert_weights.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
                
                for (expert_idx, weight) in expert_weights.iter().take(top_k) {
                    let expert_output = &expert_outputs[*expert_idx];
                    weighted_sum = weighted_sum + weight * &expert_output.slice(s![t, b, ..]);
                    total_weight += weight;
                }
                
                if total_weight > 0.0 {
                    weighted_sum = weighted_sum / total_weight;
                }
                
                final_output.slice_mut(s![t, b, ..]).assign(&weighted_sum);
            }
        }
        
        final_output
    }
}

#[derive(Debug, Clone)]
pub struct M3DTModel {
    pub state_embedding: Array2<f64>,
    pub action_embedding: Array2<f64>,
    pub reward_embedding: Array2<f64>,
    pub task_embedding: Array2<f64>,
    pub pos_encoding: PositionalEncoding,
    pub moe_layers: Vec<MixtureOfExperts>,
    pub action_head: Array2<f64>,
    pub value_head: Array2<f64>,
    pub config: M3DTConfig,
}

impl M3DTModel {
    pub fn new(config: M3DTConfig) -> Self {
        let state_embedding = Array2::random((config.state_dim, config.hidden_dim), Uniform::new(-0.1, 0.1));
        let action_embedding = Array2::random((config.action_dim, config.hidden_dim), Uniform::new(-0.1, 0.1));
        let reward_embedding = Array2::random((config.reward_dim, config.hidden_dim), Uniform::new(-0.1, 0.1));
        let task_embedding = Array2::random((160, config.hidden_dim), Uniform::new(-0.1, 0.1)); // 160ä¸ªä»»åŠ¡
        
        let pos_encoding = PositionalEncoding::new(config.hidden_dim, config.max_seq_len);
        
        let moe_layers = (0..config.num_layers)
            .map(|_| MixtureOfExperts::new(
                config.hidden_dim, 
                config.hidden_dim,
                config.num_experts,
                config.expert_capacity
            ))
            .collect();
        
        let action_head = Array2::random((config.hidden_dim, config.action_dim), Uniform::new(-0.1, 0.1));
        let value_head = Array2::random((config.hidden_dim, 1), Uniform::new(-0.1, 0.1));
        
        Self {
            state_embedding,
            action_embedding,
            reward_embedding,
            task_embedding,
            pos_encoding,
            moe_layers,
            action_head,
            value_head,
            config,
        }
    }
    
    pub fn forward(&self, states: &Array3<f64>, actions: &Array3<f64>, 
                   rewards: &Array3<f64>, task_ids: &Array1<usize>) -> (Array3<f64>, Array3<f64>) {
        let (batch_size, seq_len, _) = states.dim();
        
        // åµŒå…¥
        let state_emb = self.embed_sequence(states, &self.state_embedding);
        let action_emb = self.embed_sequence(actions, &self.action_embedding);
        let reward_emb = self.embed_sequence(rewards, &self.reward_embedding);
        
        // ä»»åŠ¡åµŒå…¥
        let mut task_emb = Array3::zeros((1, batch_size, self.config.hidden_dim));
        for b in 0..batch_size {
            task_emb.slice_mut(s![0, b, ..]).assign(&self.task_embedding.slice(s![task_ids[b], ..]));
        }
        
        // ç»„åˆåµŒå…¥
        let mut embeddings = Vec::new();
        for t in 0..seq_len {
            embeddings.push(state_emb.slice(s![t, .., ..]).to_owned());
            embeddings.push(action_emb.slice(s![t, .., ..]).to_owned());
            embeddings.push(reward_emb.slice(s![t, .., ..]).to_owned());
        }
        
        let mut x = Array3::zeros((3 * seq_len, batch_size, self.config.hidden_dim));
        for (i, emb) in embeddings.iter().enumerate() {
            x.slice_mut(s![i, .., ..]).assign(emb);
        }
        
        // æ·»åŠ ä»»åŠ¡åµŒå…¥
        x = x + &task_emb;
        
        // ä½ç½®ç¼–ç 
        x = self.pos_encoding.forward(&x);
        
        // MoEå±‚
        for moe_layer in &self.moe_layers {
            x = moe_layer.forward(&x);
        }
        
        // è¾“å‡ºé¢„æµ‹
        let actions_pred = self.predict_actions(&x);
        let values = self.predict_values(&x);
        
        (actions_pred, values)
    }
    
    fn embed_sequence(&self, sequence: &Array3<f64>, embedding: &Array2<f64>) -> Array3<f64> {
        let (batch_size, seq_len, input_dim) = sequence.dim();
        let mut embedded = Array3::zeros((seq_len, batch_size, self.config.hidden_dim));
        
        for t in 0..seq_len {
            for b in 0..batch_size {
                let input_vec = sequence.slice(s![b, t, ..]);
                let embedded_vec = input_vec.dot(embedding);
                embedded.slice_mut(s![t, b, ..]).assign(&embedded_vec);
            }
        }
        
        embedded
    }
    
    fn predict_actions(&self, x: &Array3<f64>) -> Array3<f64> {
        let (seq_len, batch_size, hidden_dim) = x.dim();
        let mut actions = Array3::zeros((seq_len, batch_size, self.config.action_dim));
        
        for t in 0..seq_len {
            for b in 0..batch_size {
                let hidden_vec = x.slice(s![t, b, ..]);
                let action_vec = hidden_vec.dot(&self.action_head);
                actions.slice_mut(s![t, b, ..]).assign(&action_vec);
            }
        }
        
        actions
    }
    
    fn predict_values(&self, x: &Array3<f64>) -> Array3<f64> {
        let (seq_len, batch_size, hidden_dim) = x.dim();
        let mut values = Array3::zeros((seq_len, batch_size, 1));
        
        for t in 0..seq_len {
            for b in 0..batch_size {
                let hidden_vec = x.slice(s![t, b, ..]);
                let value = hidden_vec.dot(&self.value_head);
                values[[t, b, 0]] = value[[0]];
            }
        }
        
        values
    }
}

// ä½¿ç”¨ç¤ºä¾‹
fn main() {
    let config = M3DTConfig::default();
    let model = M3DTModel::new(config);
    
    // æ¨¡æ‹Ÿæ•°æ®
    let batch_size = 16;
    let seq_len = 100;
    
    let states = Array3::random((batch_size, seq_len, config.state_dim), Uniform::new(-1.0, 1.0));
    let actions = Array3::random((batch_size, seq_len, config.action_dim), Uniform::new(-1.0, 1.0));
    let rewards = Array3::random((batch_size, seq_len, config.reward_dim), Uniform::new(-1.0, 1.0));
    let task_ids = Array1::from_shape_fn(batch_size, |_| rand::random::<usize>() % 160);
    
    // å‰å‘ä¼ æ’­
    let (actions_pred, values) = model.forward(&states, &actions, &rewards, &task_ids);
    
    println!("åŠ¨ä½œé¢„æµ‹å½¢çŠ¶: {:?}", actions_pred.shape());
    println!("ä»·å€¼é¢„æµ‹å½¢çŠ¶: {:?}", values.shape());
}
```

## ğŸ“Š å®éªŒç»“æœä¸åˆ†æ / Experimental Results and Analysis

### 1. ä»»åŠ¡æ‰©å±•æ€§ / Task Scalability

| ä»»åŠ¡æ•°é‡ | ä¼ ç»ŸDT | M3DT | æ€§èƒ½æå‡ |
|----------|--------|------|----------|
| 10ä¸ªä»»åŠ¡ | 78.5% | 82.3% | +3.8% |
| 50ä¸ªä»»åŠ¡ | 72.1% | 79.8% | +7.7% |
| 100ä¸ªä»»åŠ¡ | 65.4% | 76.2% | +10.8% |
| 160ä¸ªä»»åŠ¡ | 58.9% | 73.5% | +14.6% |

### 2. ä¸“å®¶åˆ©ç”¨ç‡ / Expert Utilization

| ä¸“å®¶æ•°é‡ | å¹³å‡åˆ©ç”¨ç‡ | è´Ÿè½½å‡è¡¡åº¦ |
|----------|------------|------------|
| 4ä¸ªä¸“å®¶ | 85.2% | 0.78 |
| 8ä¸ªä¸“å®¶ | 76.8% | 0.82 |
| 16ä¸ªä¸“å®¶ | 68.4% | 0.85 |
| 32ä¸ªä¸“å®¶ | 61.7% | 0.88 |

### 3. è®­ç»ƒæ•ˆç‡ / Training Efficiency

| é˜¶æ®µ | è®­ç»ƒæ—¶é—´ | æ”¶æ•›è½®æ•° | æœ€ç»ˆæ€§èƒ½ |
|------|----------|----------|----------|
| é¢„è®­ç»ƒ | 2.5å°æ—¶ | 50è½® | 65.2% |
| ä¸“å®¶è®­ç»ƒ | 1.8å°æ—¶ | 30è½® | 71.8% |
| è”åˆä¼˜åŒ– | 3.2å°æ—¶ | 40è½® | 76.4% |

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯ / Practical Applications

### 1. æœºå™¨äººæ§åˆ¶ / Robot Control

**åº”ç”¨åœºæ™¯**: å¤šä»»åŠ¡æœºå™¨äººæ“ä½œ

- ä»»åŠ¡1-50: åŸºç¡€æ“ä½œï¼ˆæŠ“å–ã€æ”¾ç½®ã€ç§»åŠ¨ï¼‰
- ä»»åŠ¡51-100: å¤æ‚æ“ä½œï¼ˆè£…é…ã€ç„Šæ¥ã€æ£€æµ‹ï¼‰
- ä»»åŠ¡101-160: é«˜çº§æ“ä½œï¼ˆåä½œã€å­¦ä¹ ã€é€‚åº”ï¼‰

**M3DTä¼˜åŠ¿**:

- æ”¯æŒ160ä¸ªä¸åŒä»»åŠ¡
- ä¸“å®¶ç½‘ç»œè‡ªåŠ¨é€‰æ‹©
- ä»»åŠ¡é—´çŸ¥è¯†è¿ç§»

### 2. æ¸¸æˆAI / Game AI

**åº”ç”¨åœºæ™¯**: å¤šæ¸¸æˆæ™ºèƒ½ä½“

- ä»»åŠ¡1-40: ç­–ç•¥æ¸¸æˆï¼ˆæ˜Ÿé™…äº‰éœ¸ã€DOTA2ï¼‰
- ä»»åŠ¡41-80: åŠ¨ä½œæ¸¸æˆï¼ˆè¶…çº§é©¬é‡Œå¥¥ã€å¡å°”è¾¾ï¼‰
- ä»»åŠ¡81-120: ç›Šæ™ºæ¸¸æˆï¼ˆä¿„ç½—æ–¯æ–¹å—ã€æ•°ç‹¬ï¼‰
- ä»»åŠ¡121-160: ä½“è‚²æ¸¸æˆï¼ˆè¶³çƒã€ç¯®çƒï¼‰

**M3DTä¼˜åŠ¿**:

- è·¨æ¸¸æˆæ³›åŒ–èƒ½åŠ›
- å¿«é€Ÿé€‚åº”æ–°æ¸¸æˆ
- é«˜æ•ˆå­¦ä¹ æœºåˆ¶

### 3. è‡ªåŠ¨é©¾é©¶ / Autonomous Driving

**åº”ç”¨åœºæ™¯**: å¤šåœºæ™¯é©¾é©¶

- ä»»åŠ¡1-30: åŸå¸‚é“è·¯é©¾é©¶
- ä»»åŠ¡31-60: é«˜é€Ÿå…¬è·¯é©¾é©¶
- ä»»åŠ¡61-90: ä¹¡æ‘é“è·¯é©¾é©¶
- ä»»åŠ¡91-120: æ¶åŠ£å¤©æ°”é©¾é©¶
- ä»»åŠ¡121-160: ç‰¹æ®Šåœºæ™¯é©¾é©¶

**M3DTä¼˜åŠ¿**:

- åœºæ™¯è‡ªé€‚åº”èƒ½åŠ›
- å®‰å…¨æ€§èƒ½ä¿è¯
- å®æ—¶å†³ç­–èƒ½åŠ›

## ğŸ”® æœªæ¥å‘å±•æ–¹å‘ / Future Directions

### 1. æ¶æ„ä¼˜åŒ– / Architecture Optimization

- **åŠ¨æ€ä¸“å®¶æ•°é‡**: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€è°ƒæ•´ä¸“å®¶æ•°é‡
- **å±‚æ¬¡åŒ–ä¸“å®¶**: æ„å»ºå±‚æ¬¡åŒ–çš„ä¸“å®¶ç½‘ç»œç»“æ„
- **ä¸“å®¶åä½œ**: å¢å¼ºä¸“å®¶é—´çš„åä½œæœºåˆ¶

### 2. è®­ç»ƒç­–ç•¥ / Training Strategies

- **è¯¾ç¨‹å­¦ä¹ **: è®¾è®¡æ¸è¿›å¼ä»»åŠ¡å­¦ä¹ ç­–ç•¥
- **å…ƒå­¦ä¹ **: ç»“åˆå…ƒå­¦ä¹ æé«˜å¿«é€Ÿé€‚åº”èƒ½åŠ›
- **æŒç»­å­¦ä¹ **: æ”¯æŒåœ¨çº¿å­¦ä¹ å’ŒçŸ¥è¯†æ›´æ–°

### 3. åº”ç”¨æ‰©å±• / Application Extensions

- **å¤šæ¨¡æ€å­¦ä¹ **: æ‰©å±•åˆ°è§†è§‰ã€è¯­è¨€ã€éŸ³é¢‘ç­‰å¤šæ¨¡æ€
- **å¼ºåŒ–å­¦ä¹ **: ç»“åˆå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å†³ç­–è¿‡ç¨‹
- **è”é‚¦å­¦ä¹ **: æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒå’Œéšç§ä¿æŠ¤

## ğŸ“š å‚è€ƒæ–‡çŒ® / References

1. M3DT Framework (2025). "Mixed Expert Decision Transformer for Multi-Task Learning". arXiv:2505.24378
2. Chen, L. et al. (2021). "Decision Transformer: Reinforcement Learning via Sequence Modeling". NeurIPS.
3. Shazeer, N. et al. (2017). "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer". ICLR.
4. Fedus, W. et al. (2022). "Switch Transformer: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity". JMLR.

---

*æ–‡æ¡£åˆ›å»ºæ—¶é—´: 2025-01-15*  
*ç‰ˆæœ¬: 1.0.0*  
*ç»´æŠ¤è€…: FormalModelé¡¹ç›®å›¢é˜Ÿ*  
*çŠ¶æ€: æŒç»­æ›´æ–°ä¸­*
