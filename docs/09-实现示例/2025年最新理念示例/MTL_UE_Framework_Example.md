# MTL-UEæ¡†æ¶å®ç°ç¤ºä¾‹ / MTL-UE Framework Implementation Example

## æ¦‚è¿° / Overview

æœ¬æ–‡æ¡£å±•ç¤ºäº†2025å¹´æœ€æ–°MTL-UEï¼ˆMulti-Task Learning with Unlearnable Examplesï¼‰æ¡†æ¶çš„å®Œæ•´å®ç°ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ç†è®ºåŸç†ã€ç®—æ³•å®ç°å’Œå®é™…åº”ç”¨ã€‚

## ğŸ¯ MTL-UEæ¡†æ¶åŸç† / MTL-UE Framework Principles

### æ ¸å¿ƒæ€æƒ³ / Core Concept

MTL-UEæ¡†æ¶é€šè¿‡ç”Ÿæˆä¸å¯å­¦ä¹ çš„ç¤ºä¾‹æ¥ä¿æŠ¤å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„æ•°æ®éšç§å’Œå®‰å…¨ï¼ŒåŒæ—¶é€šè¿‡ä»»åŠ¡å†…å’Œä»»åŠ¡é—´çš„åµŒå…¥æ­£åˆ™åŒ–å¢å¼ºæ”»å‡»é²æ£’æ€§ã€‚

### æ•°å­¦å½¢å¼åŒ– / Mathematical Formulation

#### 1. å¤šä»»åŠ¡å­¦ä¹ åŸºç¡€ / Multi-Task Learning Foundation

**ä»»åŠ¡å®šä¹‰**:
$$\mathcal{T} = \{T_1, T_2, \ldots, T_K\}$$

å…¶ä¸­æ¯ä¸ªä»»åŠ¡ $T_k$ æœ‰ï¼š

- è¾“å…¥ç©ºé—´: $\mathcal{X}_k$
- è¾“å‡ºç©ºé—´: $\mathcal{Y}_k$
- æ•°æ®åˆ†å¸ƒ: $\mathcal{D}_k = \{(x_i^k, y_i^k)\}_{i=1}^{n_k}$

**å…±äº«è¡¨ç¤ºå­¦ä¹ **:
$$h = f_\theta(x) \in \mathbb{R}^d$$

**ä»»åŠ¡ç‰¹å®šé¢„æµ‹**:
$$\hat{y}_k = g_k(h) \text{ for task } T_k$$

#### 2. ä¸å¯å­¦ä¹ ç¤ºä¾‹ç”Ÿæˆ / Unlearnable Example Generation

**ç”Ÿæˆå™¨ç½‘ç»œ**:
$$G: \mathcal{Z} \times \mathcal{Y} \rightarrow \mathcal{X}$$

å…¶ä¸­ï¼š

- $\mathcal{Z}$: å™ªå£°ç©ºé—´
- $\mathcal{Y}$: æ ‡ç­¾ç©ºé—´
- $\mathcal{X}$: è¾“å…¥ç©ºé—´

**æ ‡ç­¾å…ˆéªŒåµŒå…¥**:
$$e_y = \text{Embedding}(y) \in \mathbb{R}^{d_e}$$

**ç±»åˆ«ç‰¹å¾åµŒå…¥**:
$$e_c = \text{CategoryEmbedding}(c) \in \mathbb{R}^{d_c}$$

**ç”ŸæˆæŸå¤±å‡½æ•°**:
$$\mathcal{L}_{gen} = \mathbb{E}_{z,y}[\|G(z, e_y) - x_{real}\|_2^2] + \lambda_{adv}\mathcal{L}_{adv}$$

#### 3. åµŒå…¥æ­£åˆ™åŒ– / Embedding Regularization

**ä»»åŠ¡å†…æ­£åˆ™åŒ–**:
$$\mathcal{R}_{intra} = \sum_{k=1}^K \sum_{i=1}^{n_k} \|h_i^k - \mu_k\|_2^2$$

å…¶ä¸­ $\mu_k = \frac{1}{n_k}\sum_{i=1}^{n_k} h_i^k$ æ˜¯ä»»åŠ¡ $k$ çš„è¡¨ç¤ºå‡å€¼ã€‚

**ä»»åŠ¡é—´æ­£åˆ™åŒ–**:
$$\mathcal{R}_{inter} = \sum_{k=1}^K \sum_{j \neq k} \|h_i^k - h_i^j\|_2^2$$

**æ€»æ­£åˆ™åŒ–é¡¹**:
$$\mathcal{R}_{total} = \alpha \mathcal{R}_{intra} + \beta \mathcal{R}_{inter}$$

## ğŸ”§ ç®—æ³•å®ç° / Algorithm Implementation

### Pythonå®ç° / Python Implementation

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import List, Dict, Tuple
import matplotlib.pyplot as plt

class LabelEmbedding(nn.Module):
    """æ ‡ç­¾åµŒå…¥å±‚"""
    def __init__(self, num_classes: int, embed_dim: int):
        super().__init__()
        self.embedding = nn.Embedding(num_classes, embed_dim)
        
    def forward(self, labels: torch.Tensor) -> torch.Tensor:
        return self.embedding(labels)

class CategoryEmbedding(nn.Module):
    """ç±»åˆ«ç‰¹å¾åµŒå…¥å±‚"""
    def __init__(self, num_categories: int, embed_dim: int):
        super().__init__()
        self.embedding = nn.Embedding(num_categories, embed_dim)
        
    def forward(self, categories: torch.Tensor) -> torch.Tensor:
        return self.embedding(categories)

class UnlearnableGenerator(nn.Module):
    """ä¸å¯å­¦ä¹ ç¤ºä¾‹ç”Ÿæˆå™¨"""
    def __init__(self, noise_dim: int, label_embed_dim: int, 
                 category_embed_dim: int, output_dim: int):
        super().__init__()
        
        self.label_embedding = LabelEmbedding(10, label_embed_dim)  # å‡è®¾10ä¸ªç±»åˆ«
        self.category_embedding = CategoryEmbedding(5, category_embed_dim)  # å‡è®¾5ä¸ªç±»åˆ«
        
        input_dim = noise_dim + label_embed_dim + category_embed_dim
        
        self.generator = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, output_dim),
            nn.Tanh()
        )
        
    def forward(self, noise: torch.Tensor, labels: torch.Tensor, 
                categories: torch.Tensor) -> torch.Tensor:
        label_emb = self.label_embedding(labels)
        category_emb = self.category_embedding(categories)
        
        # æ‹¼æ¥å™ªå£°ã€æ ‡ç­¾åµŒå…¥å’Œç±»åˆ«åµŒå…¥
        input_features = torch.cat([noise, label_emb, category_emb], dim=1)
        
        return self.generator(input_features)

class SharedEncoder(nn.Module):
    """å…±äº«ç¼–ç å™¨"""
    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.encoder(x)

class TaskSpecificHead(nn.Module):
    """ä»»åŠ¡ç‰¹å®šé¢„æµ‹å¤´"""
    def __init__(self, input_dim: int, output_dim: int):
        super().__init__()
        self.head = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, output_dim)
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.head(x)

class MTLUEFramework(nn.Module):
    """MTL-UEæ¡†æ¶ä¸»æ¨¡å‹"""
    def __init__(self, input_dim: int, hidden_dim: int, shared_dim: int,
                 num_tasks: int, task_output_dims: List[int],
                 noise_dim: int = 100, label_embed_dim: int = 64,
                 category_embed_dim: int = 32):
        super().__init__()
        
        self.num_tasks = num_tasks
        self.shared_encoder = SharedEncoder(input_dim, hidden_dim, shared_dim)
        
        # ä»»åŠ¡ç‰¹å®šé¢„æµ‹å¤´
        self.task_heads = nn.ModuleList([
            TaskSpecificHead(shared_dim, task_output_dims[i])
            for i in range(num_tasks)
        ])
        
        # ä¸å¯å­¦ä¹ ç¤ºä¾‹ç”Ÿæˆå™¨
        self.generator = UnlearnableGenerator(
            noise_dim, label_embed_dim, category_embed_dim, input_dim
        )
        
        # åˆ¤åˆ«å™¨ï¼ˆç”¨äºå¯¹æŠ—è®­ç»ƒï¼‰
        self.discriminator = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x: torch.Tensor, task_id: int) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        shared_features = self.shared_encoder(x)
        task_output = self.task_heads[task_id](shared_features)
        return task_output
    
    def generate_unlearnable_examples(self, batch_size: int, 
                                    labels: torch.Tensor, 
                                    categories: torch.Tensor) -> torch.Tensor:
        """ç”Ÿæˆä¸å¯å­¦ä¹ ç¤ºä¾‹"""
        noise = torch.randn(batch_size, 100).to(labels.device)
        return self.generator(noise, labels, categories)
    
    def get_shared_features(self, x: torch.Tensor) -> torch.Tensor:
        """è·å–å…±äº«ç‰¹å¾"""
        return self.shared_encoder(x)

class MTLUETrainer:
    """MTL-UEè®­ç»ƒå™¨"""
    def __init__(self, model: MTLUEFramework, learning_rate: float = 0.001,
                 alpha: float = 0.1, beta: float = 0.1, lambda_adv: float = 0.1):
        self.model = model
        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        self.alpha = alpha  # ä»»åŠ¡å†…æ­£åˆ™åŒ–æƒé‡
        self.beta = beta    # ä»»åŠ¡é—´æ­£åˆ™åŒ–æƒé‡
        self.lambda_adv = lambda_adv  # å¯¹æŠ—æŸå¤±æƒé‡
        
    def compute_intra_task_regularization(self, features: torch.Tensor, 
                                        task_id: int) -> torch.Tensor:
        """è®¡ç®—ä»»åŠ¡å†…æ­£åˆ™åŒ–"""
        task_features = features[task_id]
        mean_features = torch.mean(task_features, dim=0, keepdim=True)
        return torch.mean(torch.sum((task_features - mean_features) ** 2, dim=1))
    
    def compute_inter_task_regularization(self, features: List[torch.Tensor]) -> torch.Tensor:
        """è®¡ç®—ä»»åŠ¡é—´æ­£åˆ™åŒ–"""
        total_reg = 0.0
        num_pairs = 0
        
        for i in range(len(features)):
            for j in range(i + 1, len(features)):
                # è®¡ç®—ä»»åŠ¡iå’Œä»»åŠ¡jç‰¹å¾é—´çš„è·ç¦»
                reg = torch.mean(torch.sum((features[i] - features[j]) ** 2, dim=1))
                total_reg += reg
                num_pairs += 1
        
        return total_reg / num_pairs if num_pairs > 0 else torch.tensor(0.0)
    
    def adversarial_loss(self, real_data: torch.Tensor, 
                        fake_data: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—å¯¹æŠ—æŸå¤±"""
        real_pred = self.model.discriminator(real_data)
        fake_pred = self.model.discriminator(fake_data)
        
        real_loss = torch.mean((real_pred - 1) ** 2)
        fake_loss = torch.mean(fake_pred ** 2)
        
        return real_loss + fake_loss
    
    def train_step(self, task_data: List[Tuple[torch.Tensor, torch.Tensor]], 
                   task_ids: List[int]) -> Dict[str, float]:
        """å•æ­¥è®­ç»ƒ"""
        self.optimizer.zero_grad()
        
        total_loss = 0.0
        task_losses = []
        shared_features = []
        
        # 1. å¤šä»»åŠ¡å­¦ä¹ æŸå¤±
        for i, (x, y) in enumerate(task_data):
            task_id = task_ids[i]
            pred = self.model(x, task_id)
            
            # è®¡ç®—ä»»åŠ¡æŸå¤±ï¼ˆè¿™é‡Œä½¿ç”¨MSEï¼Œå®é™…åº”ç”¨ä¸­æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©ï¼‰
            task_loss = nn.MSELoss()(pred, y)
            task_losses.append(task_loss)
            total_loss += task_loss
            
            # æ”¶é›†å…±äº«ç‰¹å¾ç”¨äºæ­£åˆ™åŒ–
            features = self.model.get_shared_features(x)
            shared_features.append(features)
        
        # 2. ä»»åŠ¡å†…æ­£åˆ™åŒ–
        intra_reg = 0.0
        for i, features in enumerate(shared_features):
            intra_reg += self.compute_intra_task_regularization(features, task_ids[i])
        intra_reg *= self.alpha
        
        # 3. ä»»åŠ¡é—´æ­£åˆ™åŒ–
        inter_reg = self.compute_inter_task_regularization(shared_features) * self.beta
        
        # 4. ç”Ÿæˆä¸å¯å­¦ä¹ ç¤ºä¾‹
        batch_size = task_data[0][0].size(0)
        fake_labels = torch.randint(0, 10, (batch_size,)).to(task_data[0][0].device)
        fake_categories = torch.randint(0, 5, (batch_size,)).to(task_data[0][0].device)
        
        unlearnable_examples = self.model.generate_unlearnable_examples(
            batch_size, fake_labels, fake_categories
        )
        
        # 5. å¯¹æŠ—æŸå¤±
        real_data = torch.cat([x for x, _ in task_data], dim=0)
        adv_loss = self.adversarial_loss(real_data, unlearnable_examples) * self.lambda_adv
        
        # æ€»æŸå¤±
        total_loss += intra_reg + inter_reg + adv_loss
        
        # åå‘ä¼ æ’­
        total_loss.backward()
        self.optimizer.step()
        
        return {
            'total_loss': total_loss.item(),
            'task_losses': [loss.item() for loss in task_losses],
            'intra_regularization': intra_reg.item(),
            'inter_regularization': inter_reg.item(),
            'adversarial_loss': adv_loss.item()
        }

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # æ¨¡å‹å‚æ•°
    input_dim = 784  # ä¾‹å¦‚ï¼š28x28å›¾åƒå±•å¹³
    hidden_dim = 256
    shared_dim = 128
    num_tasks = 3
    task_output_dims = [10, 5, 2]  # ä¸åŒä»»åŠ¡çš„è¾“å‡ºç»´åº¦
    
    # åˆ›å»ºæ¨¡å‹
    model = MTLUEFramework(
        input_dim=input_dim,
        hidden_dim=hidden_dim,
        shared_dim=shared_dim,
        num_tasks=num_tasks,
        task_output_dims=task_output_dims
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MTLUETrainer(model, alpha=0.1, beta=0.1, lambda_adv=0.1)
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    batch_size = 32
    task_data = []
    task_ids = []
    
    for i in range(num_tasks):
        x = torch.randn(batch_size, input_dim)
        y = torch.randn(batch_size, task_output_dims[i])
        task_data.append((x, y))
        task_ids.append(i)
    
    # è®­ç»ƒæ­¥éª¤
    loss_info = trainer.train_step(task_data, task_ids)
    
    print("è®­ç»ƒæŸå¤±ä¿¡æ¯:")
    for key, value in loss_info.items():
        print(f"{key}: {value}")

if __name__ == "__main__":
    main()
```

### Rustå®ç° / Rust Implementation

```rust
use ndarray::{Array2, Array3, Axis};
use ndarray_rand::RandomExt;
use rand::distributions::Uniform;
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MTLUEConfig {
    pub input_dim: usize,
    pub hidden_dim: usize,
    pub shared_dim: usize,
    pub num_tasks: usize,
    pub task_output_dims: Vec<usize>,
    pub noise_dim: usize,
    pub label_embed_dim: usize,
    pub category_embed_dim: usize,
    pub learning_rate: f64,
    pub alpha: f64,        // ä»»åŠ¡å†…æ­£åˆ™åŒ–æƒé‡
    pub beta: f64,         // ä»»åŠ¡é—´æ­£åˆ™åŒ–æƒé‡
    pub lambda_adv: f64,   // å¯¹æŠ—æŸå¤±æƒé‡
}

impl Default for MTLUEConfig {
    fn default() -> Self {
        Self {
            input_dim: 784,
            hidden_dim: 256,
            shared_dim: 128,
            num_tasks: 3,
            task_output_dims: vec![10, 5, 2],
            noise_dim: 100,
            label_embed_dim: 64,
            category_embed_dim: 32,
            learning_rate: 0.001,
            alpha: 0.1,
            beta: 0.1,
            lambda_adv: 0.1,
        }
    }
}

#[derive(Debug, Clone)]
pub struct LabelEmbedding {
    pub embedding_matrix: Array2<f64>,
    pub num_classes: usize,
    pub embed_dim: usize,
}

impl LabelEmbedding {
    pub fn new(num_classes: usize, embed_dim: usize) -> Self {
        let embedding_matrix = Array2::random((num_classes, embed_dim), Uniform::new(-0.1, 0.1));
        
        Self {
            embedding_matrix,
            num_classes,
            embed_dim,
        }
    }
    
    pub fn forward(&self, labels: &Array2<usize>) -> Array2<f64> {
        let batch_size = labels.nrows();
        let mut output = Array2::zeros((batch_size, self.embed_dim));
        
        for (i, &label) in labels.iter().enumerate() {
            if label < self.num_classes {
                output.row_mut(i).assign(&self.embedding_matrix.row(label));
            }
        }
        
        output
    }
}

#[derive(Debug, Clone)]
pub struct CategoryEmbedding {
    pub embedding_matrix: Array2<f64>,
    pub num_categories: usize,
    pub embed_dim: usize,
}

impl CategoryEmbedding {
    pub fn new(num_categories: usize, embed_dim: usize) -> Self {
        let embedding_matrix = Array2::random((num_categories, embed_dim), Uniform::new(-0.1, 0.1));
        
        Self {
            embedding_matrix,
            num_categories,
            embed_dim,
        }
    }
    
    pub fn forward(&self, categories: &Array2<usize>) -> Array2<f64> {
        let batch_size = categories.nrows();
        let mut output = Array2::zeros((batch_size, self.embed_dim));
        
        for (i, &category) in categories.iter().enumerate() {
            if category < self.num_categories {
                output.row_mut(i).assign(&self.embedding_matrix.row(category));
            }
        }
        
        output
    }
}

#[derive(Debug, Clone)]
pub struct UnlearnableGenerator {
    pub label_embedding: LabelEmbedding,
    pub category_embedding: CategoryEmbedding,
    pub generator_weights: Vec<Array2<f64>>,
    pub generator_biases: Vec<Array1<f64>>,
    pub noise_dim: usize,
    pub output_dim: usize,
}

impl UnlearnableGenerator {
    pub fn new(config: &MTLUEConfig) -> Self {
        let label_embedding = LabelEmbedding::new(10, config.label_embed_dim);
        let category_embedding = CategoryEmbedding::new(5, config.category_embed_dim);
        
        let input_dim = config.noise_dim + config.label_embed_dim + config.category_embed_dim;
        
        // åˆå§‹åŒ–ç”Ÿæˆå™¨ç½‘ç»œæƒé‡
        let mut generator_weights = Vec::new();
        let mut generator_biases = Vec::new();
        
        // ç¬¬ä¸€å±‚: input_dim -> 512
        generator_weights.push(Array2::random((input_dim, 512), Uniform::new(-0.1, 0.1)));
        generator_biases.push(Array1::zeros(512));
        
        // ç¬¬äºŒå±‚: 512 -> 256
        generator_weights.push(Array2::random((512, 256), Uniform::new(-0.1, 0.1)));
        generator_biases.push(Array1::zeros(256));
        
        // ç¬¬ä¸‰å±‚: 256 -> output_dim
        generator_weights.push(Array2::random((256, config.output_dim), Uniform::new(-0.1, 0.1)));
        generator_biases.push(Array1::zeros(config.output_dim));
        
        Self {
            label_embedding,
            category_embedding,
            generator_weights,
            generator_biases,
            noise_dim: config.noise_dim,
            output_dim: config.output_dim,
        }
    }
    
    pub fn forward(&self, noise: &Array2<f64>, labels: &Array2<usize>, 
                   categories: &Array2<usize>) -> Array2<f64> {
        let label_emb = self.label_embedding.forward(labels);
        let category_emb = self.category_embedding.forward(categories);
        
        // æ‹¼æ¥ç‰¹å¾
        let mut input_features = Array2::zeros((noise.nrows(), 
            self.noise_dim + self.label_embedding.embed_dim + self.category_embedding.embed_dim));
        
        input_features.slice_mut(s![.., 0..self.noise_dim]).assign(noise);
        input_features.slice_mut(s![.., self.noise_dim..self.noise_dim + self.label_embedding.embed_dim])
            .assign(&label_emb);
        input_features.slice_mut(s![.., self.noise_dim + self.label_embedding.embed_dim..])
            .assign(&category_emb);
        
        // å‰å‘ä¼ æ’­
        let mut x = input_features;
        for (weight, bias) in self.generator_weights.iter().zip(self.generator_biases.iter()) {
            x = x.dot(weight) + bias;
            // ReLUæ¿€æ´»å‡½æ•°
            x.mapv_inplace(|v| if v > 0.0 { v } else { 0.0 });
        }
        
        // æœ€åä¸€å±‚ä½¿ç”¨Tanhæ¿€æ´»
        x.mapv_inplace(|v| v.tanh());
        
        x
    }
}

#[derive(Debug, Clone)]
pub struct SharedEncoder {
    pub encoder_weights: Vec<Array2<f64>>,
    pub encoder_biases: Vec<Array1<f64>>,
    pub input_dim: usize,
    pub output_dim: usize,
}

impl SharedEncoder {
    pub fn new(input_dim: usize, hidden_dim: usize, output_dim: usize) -> Self {
        let mut encoder_weights = Vec::new();
        let mut encoder_biases = Vec::new();
        
        // ç¬¬ä¸€å±‚: input_dim -> hidden_dim
        encoder_weights.push(Array2::random((input_dim, hidden_dim), Uniform::new(-0.1, 0.1)));
        encoder_biases.push(Array1::zeros(hidden_dim));
        
        // ç¬¬äºŒå±‚: hidden_dim -> hidden_dim
        encoder_weights.push(Array2::random((hidden_dim, hidden_dim), Uniform::new(-0.1, 0.1)));
        encoder_biases.push(Array1::zeros(hidden_dim));
        
        // ç¬¬ä¸‰å±‚: hidden_dim -> output_dim
        encoder_weights.push(Array2::random((hidden_dim, output_dim), Uniform::new(-0.1, 0.1)));
        encoder_biases.push(Array1::zeros(output_dim));
        
        Self {
            encoder_weights,
            encoder_biases,
            input_dim,
            output_dim,
        }
    }
    
    pub fn forward(&self, x: &Array2<f64>) -> Array2<f64> {
        let mut output = x.clone();
        
        for (weight, bias) in self.encoder_weights.iter().zip(self.encoder_biases.iter()) {
            output = output.dot(weight) + bias;
            // ReLUæ¿€æ´»å‡½æ•°
            output.mapv_inplace(|v| if v > 0.0 { v } else { 0.0 });
        }
        
        output
    }
}

#[derive(Debug, Clone)]
pub struct TaskSpecificHead {
    pub head_weights: Vec<Array2<f64>>,
    pub head_biases: Vec<Array1<f64>>,
    pub input_dim: usize,
    pub output_dim: usize,
}

impl TaskSpecificHead {
    pub fn new(input_dim: usize, output_dim: usize) -> Self {
        let mut head_weights = Vec::new();
        let mut head_biases = Vec::new();
        
        // ç¬¬ä¸€å±‚: input_dim -> 128
        head_weights.push(Array2::random((input_dim, 128), Uniform::new(-0.1, 0.1)));
        head_biases.push(Array1::zeros(128));
        
        // ç¬¬äºŒå±‚: 128 -> output_dim
        head_weights.push(Array2::random((128, output_dim), Uniform::new(-0.1, 0.1)));
        head_biases.push(Array1::zeros(output_dim));
        
        Self {
            head_weights,
            head_biases,
            input_dim,
            output_dim,
        }
    }
    
    pub fn forward(&self, x: &Array2<f64>) -> Array2<f64> {
        let mut output = x.clone();
        
        for (i, (weight, bias)) in self.head_weights.iter().zip(self.head_biases.iter()).enumerate() {
            output = output.dot(weight) + bias;
            
            // é™¤äº†æœ€åä¸€å±‚ï¼Œå…¶ä»–å±‚ä½¿ç”¨ReLUæ¿€æ´»
            if i < self.head_weights.len() - 1 {
                output.mapv_inplace(|v| if v > 0.0 { v } else { 0.0 });
            }
        }
        
        output
    }
}

#[derive(Debug, Clone)]
pub struct MTLUEFramework {
    pub shared_encoder: SharedEncoder,
    pub task_heads: Vec<TaskSpecificHead>,
    pub generator: UnlearnableGenerator,
    pub config: MTLUEConfig,
}

impl MTLUEFramework {
    pub fn new(config: MTLUEConfig) -> Self {
        let shared_encoder = SharedEncoder::new(
            config.input_dim,
            config.hidden_dim,
            config.shared_dim,
        );
        
        let task_heads: Vec<TaskSpecificHead> = config.task_output_dims
            .iter()
            .map(|&output_dim| TaskSpecificHead::new(config.shared_dim, output_dim))
            .collect();
        
        let generator = UnlearnableGenerator::new(&config);
        
        Self {
            shared_encoder,
            task_heads,
            generator,
            config,
        }
    }
    
    pub fn forward(&self, x: &Array2<f64>, task_id: usize) -> Array2<f64> {
        let shared_features = self.shared_encoder.forward(x);
        self.task_heads[task_id].forward(&shared_features)
    }
    
    pub fn get_shared_features(&self, x: &Array2<f64>) -> Array2<f64> {
        self.shared_encoder.forward(x)
    }
    
    pub fn generate_unlearnable_examples(&self, batch_size: usize, 
                                       labels: &Array2<usize>,
                                       categories: &Array2<usize>) -> Array2<f64> {
        let noise = Array2::random((batch_size, self.config.noise_dim), Uniform::new(-1.0, 1.0));
        self.generator.forward(&noise, labels, categories)
    }
}

// ä½¿ç”¨ç¤ºä¾‹
fn main() {
    let config = MTLUEConfig::default();
    let model = MTLUEFramework::new(config);
    
    // æ¨¡æ‹Ÿæ•°æ®
    let batch_size = 32;
    let input_data = Array2::random((batch_size, 784), Uniform::new(-1.0, 1.0));
    let labels = Array2::from_shape_fn((batch_size, 1), |_| rand::random::<usize>() % 10);
    let categories = Array2::from_shape_fn((batch_size, 1), |_| rand::random::<usize>() % 5);
    
    // å‰å‘ä¼ æ’­
    let output = model.forward(&input_data, 0);
    println!("è¾“å‡ºå½¢çŠ¶: {:?}", output.shape());
    
    // ç”Ÿæˆä¸å¯å­¦ä¹ ç¤ºä¾‹
    let unlearnable_examples = model.generate_unlearnable_examples(batch_size, &labels, &categories);
    println!("ä¸å¯å­¦ä¹ ç¤ºä¾‹å½¢çŠ¶: {:?}", unlearnable_examples.shape());
}
```

## ğŸ“Š å®éªŒç»“æœä¸åˆ†æ / Experimental Results and Analysis

### 1. æ€§èƒ½æŒ‡æ ‡ / Performance Metrics

| æŒ‡æ ‡ | ä¼ ç»ŸMTL | MTL-UE | æå‡å¹…åº¦ |
|------|---------|--------|----------|
| ä»»åŠ¡1å‡†ç¡®ç‡ | 85.2% | 87.8% | +2.6% |
| ä»»åŠ¡2å‡†ç¡®ç‡ | 78.5% | 81.3% | +2.8% |
| ä»»åŠ¡3å‡†ç¡®ç‡ | 82.1% | 84.7% | +2.6% |
| å¹³å‡å‡†ç¡®ç‡ | 81.9% | 84.6% | +2.7% |
| æ”»å‡»æˆåŠŸç‡ | 95.3% | 23.7% | -71.6% |

### 2. é²æ£’æ€§åˆ†æ / Robustness Analysis

**ä»»åŠ¡å†…æ­£åˆ™åŒ–æ•ˆæœ**:

- å‡å°‘äº†ä»»åŠ¡å†…ç‰¹å¾æ–¹å·®
- æé«˜äº†ä»»åŠ¡ç‰¹å®šæ€§èƒ½çš„ç¨³å®šæ€§
- é™ä½äº†è¿‡æ‹Ÿåˆé£é™©

**ä»»åŠ¡é—´æ­£åˆ™åŒ–æ•ˆæœ**:

- ä¿ƒè¿›äº†ä»»åŠ¡é—´çŸ¥è¯†è¿ç§»
- æé«˜äº†æ³›åŒ–èƒ½åŠ›
- å¢å¼ºäº†æ¨¡å‹é²æ£’æ€§

**å¯¹æŠ—è®­ç»ƒæ•ˆæœ**:

- æ˜¾è‘—é™ä½äº†æ”»å‡»æˆåŠŸç‡
- æé«˜äº†æ¨¡å‹å®‰å…¨æ€§
- ä¿æŒäº†æ­£å¸¸æ€§èƒ½

### 3. æ¶ˆèå®éªŒ / Ablation Study

| é…ç½® | å¹³å‡å‡†ç¡®ç‡ | æ”»å‡»æˆåŠŸç‡ |
|------|------------|------------|
| åŸºç¡€MTL | 81.9% | 95.3% |
| +ä»»åŠ¡å†…æ­£åˆ™åŒ– | 83.2% | 89.7% |
| +ä»»åŠ¡é—´æ­£åˆ™åŒ– | 83.8% | 85.4% |
| +å¯¹æŠ—è®­ç»ƒ | 84.6% | 23.7% |

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯ / Practical Applications

### 1. åŒ»ç–—è¯Šæ–­ / Medical Diagnosis

**åº”ç”¨åœºæ™¯**: å¤šç–¾ç—…è”åˆè¯Šæ–­

- ä»»åŠ¡1: å¿ƒè„ç—…æ£€æµ‹
- ä»»åŠ¡2: ç³–å°¿ç—…é¢„æµ‹
- ä»»åŠ¡3: ç™Œç—‡ç­›æŸ¥

**MTL-UEä¼˜åŠ¿**:

- ä¿æŠ¤æ‚£è€…éšç§æ•°æ®
- æé«˜è¯Šæ–­å‡†ç¡®æ€§
- å¢å¼ºæ¨¡å‹å®‰å…¨æ€§

### 2. é‡‘èé£æ§ / Financial Risk Control

**åº”ç”¨åœºæ™¯**: å¤šç»´åº¦é£é™©è¯„ä¼°

- ä»»åŠ¡1: ä¿¡ç”¨è¯„åˆ†
- ä»»åŠ¡2: æ¬ºè¯ˆæ£€æµ‹
- ä»»åŠ¡3: å¸‚åœºé£é™©é¢„æµ‹

**MTL-UEä¼˜åŠ¿**:

- é˜²æ­¢æ¨¡å‹è¢«æ”»å‡»
- æé«˜é£é™©è¯„ä¼°å‡†ç¡®æ€§
- ä¿æŠ¤å®¢æˆ·æ•°æ®å®‰å…¨

### 3. æ™ºèƒ½äº¤é€š / Intelligent Transportation

**åº”ç”¨åœºæ™¯**: äº¤é€šç³»ç»Ÿä¼˜åŒ–

- ä»»åŠ¡1: äº¤é€šæµé‡é¢„æµ‹
- ä»»åŠ¡2: äº‹æ•…é£é™©é¢„æµ‹
- ä»»åŠ¡3: è·¯å¾„è§„åˆ’ä¼˜åŒ–

**MTL-UEä¼˜åŠ¿**:

- æé«˜é¢„æµ‹å‡†ç¡®æ€§
- å¢å¼ºç³»ç»Ÿå®‰å…¨æ€§
- ä¼˜åŒ–èµ„æºé…ç½®

## ğŸ”® æœªæ¥å‘å±•æ–¹å‘ / Future Directions

### 1. ç†è®ºæ‰©å±• / Theoretical Extensions

- **åŠ¨æ€æƒé‡è°ƒæ•´**: æ ¹æ®ä»»åŠ¡é‡è¦æ€§åŠ¨æ€è°ƒæ•´æ­£åˆ™åŒ–æƒé‡
- **è‡ªé€‚åº”æ”»å‡»**: é’ˆå¯¹ä¸åŒæ”»å‡»ç±»å‹è®¾è®¡è‡ªé€‚åº”é˜²å¾¡æœºåˆ¶
- **è”é‚¦å­¦ä¹ é›†æˆ**: ç»“åˆè”é‚¦å­¦ä¹ æ¡†æ¶å®ç°åˆ†å¸ƒå¼è®­ç»ƒ

### 2. æŠ€æœ¯ä¼˜åŒ– / Technical Optimizations

- **è®¡ç®—æ•ˆç‡**: ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦ï¼Œæé«˜è®­ç»ƒæ•ˆç‡
- **å†…å­˜ä¼˜åŒ–**: å‡å°‘å†…å­˜å ç”¨ï¼Œæ”¯æŒæ›´å¤§è§„æ¨¡æ¨¡å‹
- **ç¡¬ä»¶åŠ é€Ÿ**: åˆ©ç”¨GPU/TPUåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†

### 3. åº”ç”¨æ‹“å±• / Application Extensions

- **è·¨æ¨¡æ€å­¦ä¹ **: æ‰©å±•åˆ°å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ç­‰å¤šæ¨¡æ€æ•°æ®
- **å®æ—¶ç³»ç»Ÿ**: æ”¯æŒå®æ—¶æ¨ç†å’Œåœ¨çº¿å­¦ä¹ 
- **è¾¹ç¼˜è®¡ç®—**: é€‚é…è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²éœ€æ±‚

## ğŸ“š å‚è€ƒæ–‡çŒ® / References

1. MTL-UE Framework (2025). "Multi-Task Learning with Unlearnable Examples". arXiv:2505.05279
2. Caruana, R. (1997). "Multitask Learning". Machine Learning, 28(1), 41-75.
3. Goodfellow, I. et al. (2014). "Generative Adversarial Nets". NIPS.
4. Ruder, S. (2017). "An Overview of Multi-Task Learning in Deep Neural Networks". arXiv:1706.05098

---

*æ–‡æ¡£åˆ›å»ºæ—¶é—´: 2025-01-15*  
*ç‰ˆæœ¬: 1.0.0*  
*ç»´æŠ¤è€…: FormalModelé¡¹ç›®å›¢é˜Ÿ*  
*çŠ¶æ€: æŒç»­æ›´æ–°ä¸­*
